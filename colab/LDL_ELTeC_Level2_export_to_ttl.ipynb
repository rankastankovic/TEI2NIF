{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Export srpELTeC to NIF\n",
        "\n",
        "* Virtual mobility Grant holder: ***Ranka Stanković ***\n",
        "* Virtual mobility grant: E-COST-GRANT-CA18209-4349b689, \"European literary text collection ELTEC transformation and publishing as linguistic linked\n",
        "open data: use case for 3 languages\"\n",
        "\n"
      ],
      "metadata": {
        "id": "eXyvhBWb8hgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Start and end date: 01/08/2022 to 15/10/2022\n",
        "* COST Action: CA18209\n",
        "* collaborator: Milica Ikonić Nešić\n",
        "* Suported by Max Ionov, Christian Chiarcos and others from WG1, Task 5\n",
        "\n",
        "**Description**: Source files are 300 novels from ELTeC collection (100 per 3 languages: sr, sl, pt) coded in XML/TEI level-2  https://github.com/COST-ELTeC/ELTeC-srp/tree/master/level2 (POS taged, lemmatised, with annotated named entities, also supplied by metadata in TeiHeader and in Wikidata https://www.wikidata.org/wiki/Wikidata:WikiProject_ELTeC)"
      ],
      "metadata": {
        "id": "8AmPafxHQEBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install & import part"
      ],
      "metadata": {
        "id": "y4SsbRmnXV7v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-RRzBvNuT7U",
        "outputId": "b1a1bd8a-2090-4a00-f33f-77341beac6b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.9 gitpython-3.1.29 smmap-5.0.0\n"
          ]
        }
      ],
      "source": [
        "# for importing/clonning repository with novels\n",
        "!pip install gitpython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install rdflib sparqlwrapper pydotplus graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWn0YPe3CXSz",
        "outputId": "5bcc9699-e117-4840-aa8a-da608f791241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting sparqlwrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from rdflib) (4.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib) (57.4.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 571 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->rdflib) (3.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from isodate->rdflib) (1.15.0)\n",
            "Installing collected packages: isodate, rdflib, sparqlwrapper\n",
            "Successfully installed isodate-0.6.1 rdflib-6.2.0 sparqlwrapper-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mkwikidata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3mQ8M5CXRQK",
        "outputId": "6644a28d-3b13-4ab2-eaa3-b124b29b97b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mkwikidata\n",
            "  Downloading mkwikidata-0.14-py2.py3-none-any.whl (3.0 kB)\n",
            "Collecting requests>=2.25.1\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->mkwikidata) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->mkwikidata) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->mkwikidata) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->mkwikidata) (2.10)\n",
            "Installing collected packages: requests, mkwikidata\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed mkwikidata-0.14 requests-2.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacyopentapioca # for NEL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMIKKz4u5XJ6",
        "outputId": "45fadf69-2f9c-40b6-d0c2-c0ed54c75320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacyopentapioca\n",
            "  Downloading spacyopentapioca-0.1.5-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from spacyopentapioca) (2.28.1)\n",
            "Requirement already satisfied: spacy>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from spacyopentapioca) (3.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->spacyopentapioca) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->spacyopentapioca) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->spacyopentapioca) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->spacyopentapioca) (2.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (0.4.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (8.1.5)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (4.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (2.4.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (1.10.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (1.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.1.4->spacyopentapioca) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.1.4->spacyopentapioca) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.1.4->spacyopentapioca) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.1.4->spacyopentapioca) (5.2.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.1.4->spacyopentapioca) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.1.4->spacyopentapioca) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.1.4->spacyopentapioca) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.1.4->spacyopentapioca) (2.0.1)\n",
            "Installing collected packages: spacyopentapioca\n",
            "Successfully installed spacyopentapioca-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os,sys,glob\n",
        "import os\n",
        "import os.path\n",
        "from os import path\n",
        "import sys\n",
        "import glob \n",
        "import locale\n",
        "import spacy\n",
        "import string\n",
        "\n",
        "# xml \n",
        "from lxml import etree\n",
        "# rdf\n",
        "import rdflib\n",
        "from rdflib import Graph\n",
        "from rdflib.namespace import RDF, RDFS, XSD, OWL, DCAT, FOAF\n",
        "from rdflib import URIRef, BNode, Literal\n",
        "import networkx as nx\n",
        "import io\n",
        "import pydotplus\n",
        "from IPython.display import display, Image\n",
        "from rdflib.tools.rdf2dot import rdf2dot\n",
        "import mkwikidata\n",
        "\n",
        "ITSRDF=rdflib.Namespace(\"http://www.w3.org/2005/11/its/rdf#\")\n",
        "NIF = rdflib.Namespace(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#\")\n",
        "# NERD= rdflib.Namespace(\"http://nerd.eurecom.fr/ontology#\")\n",
        "DC = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\") \n",
        "DCT = rdflib.Namespace(\"http://purl.org/dc/terms/\") \n",
        "MS = rdflib.Namespace(\"http://w3id.org/meta-share/meta-share/\") \n",
        "WD = rdflib.Namespace(\"http://www.wikidata.org/entity/\")\n",
        "WDT = rdflib.Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
        "DBO = rdflib.Namespace(\"https://dbpedia.org/ontology/\")\n",
        "OLIA = rdflib.Namespace(\"http://purl.org/olia/discourse/olia_discourse.owl#\")\n",
        "prov_uri = URIRef(\"http://llod.jerteh.rs/ELTEC/srp/NIF/\")\n",
        "\n",
        "\n",
        "\n",
        "nlp_nel = spacy.blank('en')\n",
        "nlp_nel.add_pipe('opentapioca')"
      ],
      "metadata": {
        "id": "iWMUTUkxBldw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858b1b49-be55-49a4-9abe-5a6c288786a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacyopentapioca.entity_linker.EntityLinker at 0x7f1b515daa90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata description"
      ],
      "metadata": {
        "id": "KZM9q_kSXpRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For metadata analysed: https://link.springer.com/chapter/10.1007/978-3-319-18818-8_20, lexmeta : https://lexbib.elex.is/wiki/LexMeta, \n",
        "https://github.com/pennyl67/LexMeta/blob/main/lexmeta.ttl\n",
        "\n",
        "Metadata for collections described in Wikidata: https://www.wikidata.org/wiki/Wikidata:WikiProject_ELTeC (first edition, printed edition, ELTeC edition)\n",
        "\n",
        "Key values prepared for this edition:\n",
        "* ID - dct:identifier\n",
        "* title - dct:title (from teiHeader : titleStmt)\n",
        "* author (name) - ms:author\n",
        "* authorQID - dc:creator,\n",
        "* novelQID - linked as edition of novel \n",
        "* publisher - dct:publisher \n",
        "* licence - ms:LicenceTerms\n",
        "* year - ms:publicationDate\n",
        "* language - dc:Language, ms:Language\n",
        "* collection - wdt:P1433 \n"
      ],
      "metadata": {
        "id": "Ebp2cIKL_OFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VzoTGYh0XjVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------"
      ],
      "metadata": {
        "id": "3fAYjuSdy84I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ontology mapping"
      ],
      "metadata": {
        "id": "vtxNpp6u21JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eXYmqDdYsji1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially used:\n",
        "https://github.com/NERD-project/nerd-ontology/blob/master/nerd.owl  but replaced with: \n",
        "http://purl.org/olia/discourse/olia_discourse.owl\n",
        "\n",
        "* http://nerd.eurecom.fr/ontology#Person  (mapped to http://purl.org/olia/discourse/olia_discourse.owl#Person)\n",
        "* http://nerd.eurecom.fr/ontology#Location (mapped to http://purl.org/olia/discourse/olia_discourse.owl#Space)\n",
        "* http://nerd.eurecom.fr/ontology#Organization  (mapped to http://purl.org/olia/discourse/olia_discourse.owl#Organization)\n",
        "* http://nerd.eurecom.fr/ontology#Event  (mapped to http://purl.org/olia/discourse/olia_discourse.owl#Event)\n",
        "* ROLE - Names of posts and job titles (profession, nobility, office, military)\n",
        "* DEMO - DEMO -  Demonyms, names of kinds of people: national, regional, political (Frenchwoman; German; Parisiens;...) \n",
        "* WORK - titles of books, songs, plays, newspaper, paintings, sculptures  and other creations\n",
        "\n",
        "Additional:\n",
        "* https://dbpedia.org/ontology/Person, https://www.wikidata.org/wiki/Q5, https://schema.org/Person\n",
        "* https://dbpedia.org/ontology/Place, https://www.wikidata.org/wiki/Q7884789, https://schema.org/Place\n",
        "* https://dbpedia.org/ontology/Organisation, https://www.wikidata.org/wiki/Q43229, https://schema.org/Organization\n",
        "* https://dbpedia.org/ontology/Event, https://www.wikidata.org/wiki/Q1656682, https://schema.org/Event\n",
        "* https://dbpedia.org/ontology/Profession, https://www.wikidata.org/wiki/Q28640, https://schema.org/Occupation\n",
        "* demonym as property only: https://dbpedia.org/ontology/demonym, https://www.wikidata.org/wiki/Q217438\n",
        "* https://dbpedia.org/ontology/Work, https://www.wikidata.org/wiki/Q386724, https://schema.org/CreativeWork, \n",
        "\n",
        "\n",
        "Rizzo, Giuseppe, Raphaël Troncy, Sebastian Hellmann, and Martin Bruemmer. \"NERD meets NIF: Lifting NLP extraction results to the linked data cloud.\" In LDOW. 2012. http://ceur-ws.org/Vol-937/ldow2012-paper-02.pdf "
      ],
      "metadata": {
        "id": "jhrIbulcwxzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General classes: Token, Sentence, NamedEntity"
      ],
      "metadata": {
        "id": "s2r8ZweIUeXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEI+RDF+LLOD literature\n",
        "\n",
        "https://content.iospress.com/articles/semantic-web/sw222859\n",
        "\n",
        "* P. Ruiz Fabo, H. Bermúdez Sabel, C. Martínez Cantón and E. González-Blanco, The diachronic Spanish sonnet corpus: TEI and linked open data encoding, data distribution, and metrical findings, Digital Scholarship in the Humanities (2020). doi:10.1093/llc/fqaa035.\n",
        "\n",
        "\n",
        "* S. Tittel, H. Bermúdez-Sabel and C. Chiarcos, Using RDFa to link text and dictionary data for medieval French, in: Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), European Language Resources Association (ELRA), 2018.\n",
        "\n",
        "* Khan, Anas Fahad, Christian Chiarcos, Thierry Declerck, Daniela Gifu, Elena González-Blanco García, Jorge Gracia, Maxim Ionov et al. \"When Linguistics Meets Web Technologies. Recent advances in Modelling Linguistic Linked Data.\" (2021).\n"
      ],
      "metadata": {
        "id": "GleaZpu6ChCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# manage reading token properties from token (xml node) and generate graf triples\n",
        "class Token:    \n",
        "\n",
        "    def __init__(self, token,cur_index):\n",
        "        #self.id = token.xpath(\"./@xml:id\",namespaces={'xml':'http://www.w3.org/XML/1998/namespace'})[0]\n",
        "        self.text = token.text\n",
        "        self.pos = token.xpath(\"./@pos\")[0]\n",
        "        self.lemma = \"\" if self.pos == \"PUNCT\" or 'lemma' not in token.attrib else token.xpath(\"./@lemma\")[0]\n",
        "        self.join = token.xpath(\"./@join\")\n",
        "        # fro NER\n",
        "        self.parent = token.getparent()\n",
        "        self.first_NER_node=False\n",
        "        if  self.parent.xpath(\"name()\")==\"rs\" :\n",
        "           if list(self.parent)[0]==token: # for NER important \n",
        "              self.first_NER_node = True\n",
        "        self.index_start=cur_index\n",
        "        self.index_end=self.index_start+len(self.text)\n",
        "\n",
        "    # create graph triples for this token\n",
        "    def init_gtoken(self,g,base_url):\n",
        "      gtoken= URIRef(base_url+\"#char={0},{1}\".format(self.index_start,self.index_end))\n",
        "      g.add( (gtoken, RDF.type, NIF.Word ) )\n",
        "      g.add( (gtoken, RDF.type, NIF.RFC5147String  ) )\n",
        "      g.add( (gtoken,RDF.type, NIF.OffsetBasedString) )\n",
        "      g.add( (gtoken, NIF.anchorOf, Literal(self.text, datatype=XSD.string)) )\n",
        "      g.add( (gtoken, NIF.beginIndex, Literal(self.index_start, datatype=XSD.nonNegativeInteger)) )\n",
        "      g.add( (gtoken, NIF.endIndex, Literal(self.index_end, datatype=XSD.nonNegativeInteger)) )\n",
        "      g.add( (gtoken, NIF.posTag, Literal(self.pos, datatype=XSD.string)))\n",
        "      if self.lemma!=\"\":\n",
        "        g.add( (gtoken, NIF.lemma, Literal(self.lemma, datatype=XSD.string)))      \n",
        "\n",
        "      return gtoken"
      ],
      "metadata": {
        "id": "qG_iyAmWDsH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manage reading sentence properties from sent (xml) and generate graph triples\n",
        "class Sentence:    \n",
        "\n",
        "    def __init__(self, sent,cur_index):\n",
        "        #self.id = sent.xpath(\"./@xml:id\",namespaces={'xml':'http://www.w3.org/XML/1998/namespace'})[0]\n",
        "        self.tokens=sent.xpath(\".//*[local-name()='w' or local-name()='pc']\")\n",
        "        self.text=\"\"\n",
        "        self.index_start=cur_index\n",
        "        \n",
        "        self.otokens = []\n",
        "        # loop all tokens in sentence\n",
        "        for token in self.tokens:\n",
        "          otoken=Token(token,cur_index)\n",
        "          self.otokens.append(otoken)\n",
        "          # concatenate to sent\n",
        "          self.text +=otoken.text      \n",
        "          cur_index+=len(otoken.text)\n",
        "          if not otoken.join:\n",
        "            self.text+=\" \"\n",
        "            cur_index+=1\n",
        "        self.index_end = cur_index   \n",
        "\n",
        "    # create graph triples for this sentence\n",
        "    def init_gsent(self,g,base_url):\n",
        "      gsent= URIRef(base_url+\"#char={0},{1}\".format(self.index_start,self.index_start+len(self.text))) # add sent ID\n",
        "      g.add( (gsent, RDF.type, NIF.Sentence) )\n",
        "      g.add( (gsent, RDF.type,NIF.Context) )\n",
        "      g.add( (gsent, RDF.type, NIF.RFC5147String) )\n",
        "      g.add( (gsent, RDF.type, NIF.OffsetBasedString) )\n",
        "      # NIF.anchorOf, NIF.endIndex later\n",
        "      g.add( (gsent, NIF.beginIndex, Literal(self.index_start, datatype=XSD.nonNegativeInteger)\t))\n",
        "\n",
        "      \n",
        "      return gsent"
      ],
      "metadata": {
        "id": "FsGB6wHJNx_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manage NE named entities: PERS LOC ORG EVENT ROLE DEMO WORK \n",
        "class NamedEntity:\n",
        "\n",
        "  def __init__(self, parent,cur_index):\n",
        "      self.index_start=cur_index\n",
        "      self.text=\"\"\n",
        "      for ne in parent.getchildren():\n",
        "         self.text +=ne.text\n",
        "         self.join = ne.xpath(\"./@join\") \n",
        "         if not self.join:\n",
        "            self.text+=\" \"\n",
        "      self.text=self.text.rstrip()      \n",
        "      self.index_end=self.index_start+len(self.text) \n",
        "      self.ne_type = parent.xpath(\"./@type\")[0] \n",
        "  \n",
        "  # create graph triples for this token\n",
        "  def init_gne(self,g, base_url):\n",
        "      gne= URIRef(base_url+\"#char={0},{1}\".format(self.index_start,self.index_end) )   \n",
        "      g.add( (gne, RDF.type, NIF.OffsetBasedString) )\n",
        "      g.add( (gne, RDF.type, NIF.EntityOccurrence  ) ) \n",
        "      g.add( (gne, NIF.anchorOf,Literal(self.text, datatype=XSD.string)));\n",
        "      g.add( (gne, NIF.beginIndex,Literal(self.index_start, datatype=XSD.nonNegativeInteger)))  \n",
        "      g.add( (gne, NIF.endIndex,Literal(self.index_end, datatype=XSD.nonNegativeInteger)))  \n",
        "      if self.ne_type in ['PERS', 'PERSON']:\n",
        "         g.add( (gne, ITSRDF.taClassRef, OLIA.Person))         \n",
        "         g.add( (gne, ITSRDF.taClassRef, WDT.Q5))\n",
        "         g.add( (gne, ITSRDF.taClassRef, DBO.Person))\n",
        "      elif self.ne_type in ['LOC', 'LOCATION', 'PLACE', 'GPE']:\n",
        "        # g.add( (gne, ITSRDF.taClassRef, NERD.Location))\n",
        "        g.add( (gne, ITSRDF.taClassRef, OLIA.Space))\n",
        "        g.add( (gne, ITSRDF.taClassRef, WDT.Q7884789))\n",
        "        g.add( (gne, ITSRDF.taClassRef, DBO.Place))\n",
        "      elif self.ne_type in ['ORG', 'ORGANISATION']:\n",
        "        g.add( (gne, ITSRDF.taClassRef, OLIA.Organization))\n",
        "        g.add( (gne, ITSRDF.taClassRef, WDT.Q43229))\n",
        "        g.add( (gne, ITSRDF.taClassRef, DBO.Organisation))\n",
        "      elif self.ne_type in ['EVENT']:\n",
        "        g.add( (gne, ITSRDF.taClassRef, OLIA.Event))\n",
        "        g.add( (gne, ITSRDF.taClassRef, WDT.Q1656682))\n",
        "        g.add( (gne, ITSRDF.taClassRef, DBO.Event))\n",
        "      elif self.ne_type in ['ROLE']:\n",
        "        # g.add( (gne, ITSRDF.taClassRef, Literal(\"<\" +self.ne_type+\">\", datatype=XSD.string ))) # just string\n",
        "        g.add( (gne, ITSRDF.taClassRef, WDT.Q28640))    # profession, not exact, it could be title as well\n",
        "        g.add( (gne, ITSRDF.taClassRef, DBO.Profession))\n",
        "      elif self.ne_type in ['DEMO']:\n",
        "        # g.add( (gne, ITSRDF.taClassRef, Literal(\"<\" +self.ne_type+\">\", datatype=XSD.string ))) # just string\n",
        "        g.add( (gne, ITSRDF.taClassRef, WDT.Q217438 ))\n",
        "        g.add( (gne, ITSRDF.taClassRef, DBO.demonym)) # check this\n",
        "      elif self.ne_type in ['WORK']:\n",
        "        # g.add( (gne, ITSRDF.taClassRef, Literal(\"<\" +self.ne_type+\">\", datatype=XSD.string ))) # just string\n",
        "        g.add( (gne, ITSRDF.taClassRef, WDT.Q386724 ))\n",
        "        g.add( (gne, ITSRDF.taClassRef, DBO.Work))\n",
        "      else :  # something not expected\n",
        "         g.add( (gne, ITSRDF.taClassRef, Literal(\"<\" +self.ne_type+\">\", datatype=XSD.string ))) \n",
        "\n",
        "      # added for Tapioca 21.10.2022\n",
        "#     nel= getTapioca(self.text)\n",
        "#      if nel != None:\n",
        "#        g.add( (gne, ITSRDF.taIdentRef, URIRef(\"http://www.wikidata.org/entity/\"+nel) ))\n",
        "      return gne     "
      ],
      "metadata": {
        "id": "zACtkTJwnvwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manage reading novel and generate graph triples\n",
        "class Novel:    \n",
        "\n",
        "    def __init__(self, file_path_name, lng):\n",
        "       self.el = etree.parse(file_path_name)\n",
        "       self.id=self.el.xpath(\"./@xml:id\",namespaces={'xml':'http://www.w3.org/XML/1998/namespace'})[0]\n",
        "       # do the senteces\n",
        "       self.sentences = self.el.xpath(\"//*[local-name()='s']\")\n",
        "       self.text=\"\"\n",
        "       self.index_start=0\n",
        "       self.lng=lng\n",
        "       self.file_name =os.path.basename(file_path_name).replace(\"-L2.xml\",\"-L2.txt\")\n",
        "       self.url=\"http://llod.jerteh.rs/ELTEC/\"+lng+\"/NIF/\" +self.file_name\n",
        "\n",
        "       #  metadata  \n",
        "       self.titleStmt=self.el.xpath(\"//*[local-name()='titleStmt']\")[0]  \n",
        "       self.title=self.titleStmt.xpath(\"*[local-name()='title']\")[0]   \n",
        "       # author from TEI file\n",
        "       self.author=self.titleStmt.xpath(\"*[local-name()='author']\")[0]     \n",
        "       self.publicationStmt=self.el.xpath(\"//*[local-name()='publicationStmt']\")[0] #publicationStmt\n",
        "       if self.publicationStmt.xpath(\"*[local-name()='publisher']\") :\n",
        "         pub = self.publicationStmt.xpath(\"*[local-name()='publisher']\")[0]  \n",
        "         self.publisher=pub.text\n",
        "       else:\n",
        "         self.publisher=\"\"\n",
        "\n",
        "    # graph initialisation for one novel \n",
        "    def  init_gnovel(self,g):      \n",
        "       # for graph\n",
        "       gnovel= URIRef(self.url) # +\"{0}_{1}\".format(0,) \n",
        "       g.add( (gnovel, RDF.type, NIF.OffsetBasedString) )\n",
        "       g.add( (gnovel, RDF.type, NIF.Context  ) ) \n",
        "       g.add( (gnovel, NIF.beginIndex, Literal(\"0\",datatype=XSD.nonNegativeInteger) )) \n",
        "       g.add( (gnovel, DCT.identifier,  Literal(self.id,datatype=XSD.string)    ))\n",
        "   \n",
        "       g.add( (gnovel, DCT.title, Literal(self.title.text,datatype=XSD.string)) )\n",
        "       # language\n",
        "       lng2 = \"sr\" if lng==\"srp\" else  \"sl\" if lng==\"slv\" else  \"pt\"\n",
        "       g.add((gnovel,DC.Language, Literal(lng2,datatype=XSD.string)))\n",
        "       g.add((gnovel,MS.Language, Literal(lng2,datatype=XSD.string)))\n",
        "       # collection\n",
        "       collectionQID= \"Q106936149\" if lng==\"srp\" else \"Q111046825\" if lng==\"slv\" else \"Q111095586\" \n",
        "       g.add((gnovel,WDT.P1433,URIRef(\"http://www.wikidata.org/entity/\"+collectionQID)))\n",
        "       # from Wikidata read \n",
        "       query_result=getWiki(self.id,lng2,collectionQID)\n",
        "       for result in query_result[\"results\"][\"bindings\"]:\n",
        "         novelQID =  URIRef( wiki2entity(result[\"novel\"][\"value\"])) if (\"novel\" in result) else None\n",
        "         authorQID = URIRef( result[\"author\"][\"value\"]) if (\"author\" in result) else None\n",
        "         year =  result[\"year\"][\"value\"] if (\"year\" in result) else None\n",
        "         licence = URIRef(wiki2entity(result[\"licence\"][\"value\"])) if (\"licence\" in result) else None\n",
        "         g.add( (gnovel, WDT.P31, WD.Q3331189) ) # is edition\n",
        "         g.add( (novelQID, WDT.P747, gnovel) ) # has edition\n",
        "         g.add( (gnovel,DC.creator,authorQID) )\n",
        "         g.add( (gnovel,MS.publicationDate, Literal(year,datatype=XSD.nonNegativeInteger)) )\n",
        "         g.add( (gnovel,MS.LicenceTerms, licence) )\n",
        "\n",
        "       g.add((gnovel,MS.author,Literal(wiki2entity(self.author.text),datatype=XSD.string)))\n",
        "        \n",
        "       if self.publisher !=\"\" :\n",
        "         g.add( (gnovel, DCT.publisher, Literal(wiki2entity(self.publisher),datatype=XSD.string)) )\n",
        "        \n",
        "       return gnovel"
      ],
      "metadata": {
        "id": "zen14hvxkttT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tqAznhP3x2GF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Uq8jTmtvx2QQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation and metadata "
      ],
      "metadata": {
        "id": "6vm07KCI2iPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\"
      ],
      "metadata": {
        "id": "QDATcr0q0Xqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wiki2entity(urlWiki):\n",
        "  return urlWiki.replace(\"http://www.wikidata.org/wiki/\",\"http://www.wikidata.org/entity/\")\n"
      ],
      "metadata": {
        "id": "vbO0btJRgKiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read wikidata metapodataka\n",
        "# novel_id - string (in wikidata \"volume\")\n",
        "# lng2: sl, sr, pt\n",
        "# collectionQID: Q111046825, Q106936149, Q111095586 \n",
        "def getWiki(novel_id, lng2, collectionQID):\n",
        "  query='''\n",
        "  SELECT DISTINCT ?novel ?author ?edition (YEAR(?date) as ?year) ?licence\n",
        "  WHERE {\n",
        "    # published in (P1433) srpELTeC collection(Q106936149)\n",
        "    ?novel wdt:P31 wd:Q7725634; # is literary work\n",
        "         wdt:P747 ?edition; # has edition  \n",
        "         wdt:P577  ?date;\n",
        "         wdt:P50 ?author.\n",
        "    OPTIONAL {?edition wdt:P275 ?licence.}\n",
        "    ?edition wdt:P478 \"'''+ novel_id +'''\".   \n",
        "    ?edition wdt:P1433 wd:'''+collectionQID+'''. # published in\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"'''+lng2+'''\". }\n",
        "  }\n",
        "  '''\n",
        "  query_result = mkwikidata.run_query(query, params={ })\n",
        "  # print(query)\n",
        "  return query_result"
      ],
      "metadata": {
        "id": "bSREwM2LyeXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test wiki read\n",
        "#getWiki(\"SRP18520\", \"sr\", \"Q106936149\")\n",
        "#getWiki(\"POR0065\", \"pt\", \"Q111095586\")"
      ],
      "metadata": {
        "id": "vDZaqluEERb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get QID for named entity using opentapioca\n",
        "# ne_text - named enetity\n",
        "# added 21.10.2022.\n",
        "# nlp_nel global object\n",
        "def getTapioca(ne_text):\n",
        "  doc = nlp_nel(ne_text)\n",
        "  for ent_nel in doc.ents:\n",
        "    return ent_nel.kb_id_   # return just first"
      ],
      "metadata": {
        "id": "R50IKpap6bsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getTapioca('Београд je Srbija'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjbHsSGi5vHr",
        "outputId": "066cf984-1db8-4b9c-f315-8c341b8366d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q3711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function to write ttl file"
      ],
      "metadata": {
        "id": "w4IpcF_3CVTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guidelines for developing NIF-based NLP services \n",
        "https://www.w3.org/2015/09/bpmlod-reports/nif-based-nlp-webservices/"
      ],
      "metadata": {
        "id": "6uAuMVPbNJqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create graph, ...\n",
        "def write_gnovel(file_path_name,lng,sent_num):\n",
        "  g = Graph()\n",
        "  g.bind('itsrdf', ITSRDF)\n",
        "  g.bind('nif', NIF)\n",
        "  g.bind('olia', OLIA)\n",
        "  g.bind('dc',DC) \n",
        "  g.bind('dct',DCT)  \n",
        "  g.bind('ms',MS)  \n",
        "  g.bind('wd', WD)\n",
        "  g.bind('wdt', WDT)\n",
        "  g.bind('dbo', DBO)\n",
        "  g.bind('eltec', ELTEC)\n",
        "\n",
        "  onovel = Novel(file_path_name,lng)\n",
        "  \n",
        "  # insert initial triples for novel\n",
        "  gnovel = onovel.init_gnovel(g)\n",
        "  gsent_before=None\n",
        "  \n",
        "  scount=0 # just for testing, remove later\n",
        "  cur_index=0 # position in text\n",
        "  for sent in onovel.sentences: # loop all sentences in one novel\n",
        "    scount+=1\n",
        "    if scount > sent_num:\n",
        "      break    # break after few sentences for test\n",
        "    osent=Sentence(sent,cur_index)\n",
        "    cur_index=osent.index_end \n",
        "    gsent = osent.init_gsent(g,onovel.url)  \n",
        "    if gsent_before != None:\n",
        "        g.add( (gsent, NIF.previousSentence, gsent_before ) )\n",
        "        g.add( (gsent_before, NIF.nextSentence, gsent ) )\n",
        "    g.add( (gsent, NIF.referenceContext, gnovel\t)) \n",
        "    gtoken_before=None \n",
        "    # loop all tokens in sentence\n",
        "    for otoken in osent.otokens:\n",
        "       gtoken = otoken.init_gtoken(g,onovel.url)\n",
        "       g.add( (gtoken, NIF.referenceContext ,gnovel\t))\n",
        "       g.add( (gtoken, NIF.sentence, gsent\t)) \n",
        "       g.add( (gsent, NIF.word, gtoken\t))\n",
        "       # relate token: previous, next\n",
        "       if gtoken_before!=None:\n",
        "         g.add((gtoken_before,NIF.nextWord,gtoken))\n",
        "         g.add((gtoken, NIF.previousWord, gtoken_before))\n",
        "       # NER\n",
        "       if  otoken.first_NER_node:\n",
        "          one=NamedEntity(otoken.parent,otoken.index_start)\n",
        "          gne = one.init_gne(g,onovel.url)\n",
        "          g.add( (gne, NIF.referenceContext ,gnovel\t))       \n",
        "       gtoken_before =gtoken\n",
        "       # end of token\n",
        "  \n",
        "    # finish sentence graph  sid,len(tokens),sent_text\n",
        "    # anchorOf / isString\n",
        "    g.add( (gsent, NIF.anchorOf  , Literal(osent.text, datatype=XSD.string)) )\n",
        "    g.add( (gsent, NIF.endIndex  , Literal(osent.index_start+len(osent.text), datatype=XSD.string)) )\n",
        "\n",
        "    # concatenate to novel\n",
        "    onovel.text+= osent.text+\" \"\n",
        "    cur_index+=1\n",
        "    gsent_before = gsent\n",
        "    # end of sentence\n",
        "  \n",
        "  # finish novel      \n",
        "  g.add( (gnovel, NIF.endIndex  , Literal(len(onovel.text.rstrip()),datatype=XSD.nonNegativeInteger)))\n",
        "  # Do we use NIF.anchorOf or NIF.isString\n",
        "  g.add( (gnovel, NIF.isString  , Literal(onovel.text.rstrip(), datatype=XSD.string)) )\n",
        "  f_txt = open(file_path_name.replace(\".xml\",\".txt\").replace(\"ELTeC-\"+lng+\"/level2/\",\"NIF-\"+lng+\"/\"), \"w\")\n",
        "  f_txt.write(onovel.text.rstrip())\n",
        "  f_txt.close()\n",
        "  # write RDF file\n",
        "  file_path_name_out=file_path_name.replace(\".xml\",\".ttl\").replace(\"ELTeC-\"+lng+\"/level2/\",\"NIF-\"+lng+\"/\")\n",
        "  g.serialize(destination=file_path_name_out)"
      ],
      "metadata": {
        "id": "a9kFlCBZR-yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4XX-58Tc86s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select language repo\n",
        "This task TEI level-2: https://github.com/COST-ELTeC/ELTeC-srp, \n",
        "https://github.com/COST-ELTeC/ELTeC-slv, \n",
        "https://github.com/COST-ELTeC/ELTeC-por. \n",
        "\n",
        "Similar structure, possible for later adaptation, also level-2: https://github.com/COST-ELTeC/ELTeC-deu, https://github.com/COST-ELTeC/ELTeC-eng, https://github.com/COST-ELTeC/ELTeC-fra, \n",
        "https://github.com/COST-ELTeC/ELTeC-hun, https://github.com/COST-ELTeC/ELTeC-pol, https://github.com/COST-ELTeC/ELTeC-rom, https://github.com/COST-ELTeC/ELTeC-spa."
      ],
      "metadata": {
        "id": "gBjqDgwGNDqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a clone of repos for 3 languages\n",
        "# languages that have sentence numbers, for other collections small code modification would be required\n",
        "lngs=[\"srp\",\"slv\",\"por\"]\n",
        "#lngs=[\"srp\"]\n",
        "for lng in lngs:\n",
        "  eltec=\"ELTeC-\"+lng\n",
        "  from git.repo.base import Repo\n",
        "  if os.path.isdir(eltec) == False:\n",
        "    Repo.clone_from(\"https://github.com/COST-ELTeC/ELTeC-\"+lng, eltec)\n",
        "\n",
        "  if path.exists('NIF-'+lng) == False:\n",
        "    os.mkdir('NIF-'+lng)\n",
        "\n"
      ],
      "metadata": {
        "id": "d87OguNgvwxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test on small portion of one file srp\n",
        "lng=\"srp\"\n",
        "ELTEC = rdflib.Namespace(\"http://llod.jerteh.rs/ELTEC/\"+lng+\"/NIF/\")\n",
        "#file_name='ELTeC-srp/level2/SRP18690_GmundenskoJezero-L2.xml'\n",
        "file_name='ELTeC-srp/level2/SRP18791_KostaB_PastirKralj-L2.xml'\n",
        "file_name='ELTeC-srp/level2/SRP19101_BorisavS_NecistaKrv-L2.xml'\n",
        "write_gnovel(file_name,lng,100)"
      ],
      "metadata": {
        "id": "9ykBfQjyUezm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test on small portion of one file slv\n",
        "lng=\"slv\"\n",
        "ELTEC = rdflib.Namespace(\"http://llod.jerteh.rs/ELTEC/\"+lng+\"/NIF/\")\n",
        "file_name='ELTeC-slv/level2/SLV00122-L2.xml'\n",
        "write_gnovel(file_name,lng,20)"
      ],
      "metadata": {
        "id": "l-t9etpcduhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test on small portion of one file por\n",
        "lng=\"por\"\n",
        "ELTEC = rdflib.Namespace(\"http://llod.jerteh.rs/ELTEC/\"+lng+\"/NIF/\")\n",
        "file_name='ELTeC-por/level2/POR0024_AlfCam_Cabinda.xml'\n",
        "file_name='ELTeC-por/level2/POR0065_MarONei_Marquesa.xml'\n",
        "el = etree.parse(file_name)\n",
        "novel_id=el.xpath(\"./@xml:id\",namespaces={'xml':'http://www.w3.org/XML/1998/namespace'})[0]\n",
        "write_gnovel(file_name,lng,1000)\n",
        "#novel_id"
      ],
      "metadata": {
        "id": "qdUlzqAXaICu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list all xml files in level-2 only and generate NIF (~2 hours per language full files)\n",
        "lng=\"slv\" # srp, slv, por\n",
        "# depending on collection language\n",
        "ELTEC = rdflib.Namespace(\"http://llod.jerteh.rs/ELTEC/\"+lng+\"/NIF/\")\n",
        "lstLevel2=sorted(glob.glob(\"ELTeC-\"+lng+\"/level2/*.xml\"), key=locale.strxfrm)\n",
        "print (len(lstLevel2))\n",
        "for inp_file in lstLevel2:\n",
        "  print(inp_file)\n",
        "  try:\n",
        "      write_gnovel(inp_file,lng,1000) # for test use just few sentences\n",
        "  except:\n",
        "     print(\"........................... An exception occurred\")\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "HdYJSnhH6sRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06730f1a-bddb-4ffe-97e5-78638e445b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "ELTeC-slv/level2/SLV00024-L2.xml\n",
            "ELTeC-slv/level2/SLV00048-L2.xml\n",
            "ELTeC-slv/level2/SLV00058-L2.xml\n",
            "ELTeC-slv/level2/SLV00072-L2.xml\n",
            "ELTeC-slv/level2/SLV00090-L2.xml\n",
            "ELTeC-slv/level2/SLV00092-L2.xml\n",
            "ELTeC-slv/level2/SLV00094-L2.xml\n",
            "ELTeC-slv/level2/SLV00098-L2.xml\n",
            "ELTeC-slv/level2/SLV00099-L2.xml\n",
            "ELTeC-slv/level2/SLV00103-L2.xml\n",
            "ELTeC-slv/level2/SLV00111-L2.xml\n",
            "ELTeC-slv/level2/SLV00112-L2.xml\n",
            "ELTeC-slv/level2/SLV00122-L2.xml\n",
            "ELTeC-slv/level2/SLV00126-L2.xml\n",
            "ELTeC-slv/level2/SLV00132-L2.xml\n",
            "ELTeC-slv/level2/SLV00135-L2.xml\n",
            "ELTeC-slv/level2/SLV00136-L2.xml\n",
            "ELTeC-slv/level2/SLV00172-L2.xml\n",
            "ELTeC-slv/level2/SLV00174-L2.xml\n",
            "ELTeC-slv/level2/SLV00187-L2.xml\n",
            "ELTeC-slv/level2/SLV00194-L2.xml\n",
            "ELTeC-slv/level2/SLV00216-L2.xml\n",
            "ELTeC-slv/level2/SLV00217-L2.xml\n",
            "ELTeC-slv/level2/SLV00227-L2.xml\n",
            "ELTeC-slv/level2/SLV00231-L2.xml\n",
            "ELTeC-slv/level2/SLV00234-L2.xml\n",
            "ELTeC-slv/level2/SLV00240-L2.xml\n",
            "ELTeC-slv/level2/SLV00273-L2.xml\n",
            "ELTeC-slv/level2/SLV00278-L2.xml\n",
            "ELTeC-slv/level2/SLV00279-L2.xml\n",
            "ELTeC-slv/level2/SLV00310-L2.xml\n",
            "ELTeC-slv/level2/SLV00324-L2.xml\n",
            "ELTeC-slv/level2/SLV00325-L2.xml\n",
            "ELTeC-slv/level2/SLV00345-L2.xml\n",
            "ELTeC-slv/level2/SLV00352-L2.xml\n",
            "ELTeC-slv/level2/SLV00355-L2.xml\n",
            "ELTeC-slv/level2/SLV00363-L2.xml\n",
            "ELTeC-slv/level2/SLV00364-L2.xml\n",
            "ELTeC-slv/level2/SLV00373-L2.xml\n",
            "ELTeC-slv/level2/SLV00391-L2.xml\n",
            "ELTeC-slv/level2/SLV00398-L2.xml\n",
            "ELTeC-slv/level2/SLV00401-L2.xml\n",
            "ELTeC-slv/level2/SLV00406-L2.xml\n",
            "ELTeC-slv/level2/SLV00410-L2.xml\n",
            "ELTeC-slv/level2/SLV00417-L2.xml\n",
            "ELTeC-slv/level2/SLV00421-L2.xml\n",
            "ELTeC-slv/level2/SLV00425-L2.xml\n",
            "ELTeC-slv/level2/SLV00452-L2.xml\n",
            "ELTeC-slv/level2/SLV00454-L2.xml\n",
            "ELTeC-slv/level2/SLV00455-L2.xml\n",
            "ELTeC-slv/level2/SLV00456-L2.xml\n",
            "ELTeC-slv/level2/SLV00458-L2.xml\n",
            "ELTeC-slv/level2/SLV00459-L2.xml\n",
            "ELTeC-slv/level2/SLV00460-L2.xml\n",
            "ELTeC-slv/level2/SLV00461-L2.xml\n",
            "ELTeC-slv/level2/SLV00473-L2.xml\n",
            "ELTeC-slv/level2/SLV00476-L2.xml\n",
            "ELTeC-slv/level2/SLV00478-L2.xml\n",
            "ELTeC-slv/level2/SLV00483-L2.xml\n",
            "ELTeC-slv/level2/SLV00496-L2.xml\n",
            "ELTeC-slv/level2/SLV00497-L2.xml\n",
            "ELTeC-slv/level2/SLV00498-L2.xml\n",
            "ELTeC-slv/level2/SLV00502-L2.xml\n",
            "ELTeC-slv/level2/SLV00506-L2.xml\n",
            "ELTeC-slv/level2/SLV00526-L2.xml\n",
            "ELTeC-slv/level2/SLV10001-L2.xml\n",
            "ELTeC-slv/level2/SLV10002-L2.xml\n",
            "ELTeC-slv/level2/SLV10003-L2.xml\n",
            "ELTeC-slv/level2/SLV10004-L2.xml\n",
            "ELTeC-slv/level2/SLV10005-L2.xml\n",
            "ELTeC-slv/level2/SLV10006-L2.xml\n",
            "ELTeC-slv/level2/SLV10007-L2.xml\n",
            "ELTeC-slv/level2/SLV10008-L2.xml\n",
            "ELTeC-slv/level2/SLV10009-L2.xml\n",
            "ELTeC-slv/level2/SLV10010-L2.xml\n",
            "ELTeC-slv/level2/SLV10011-L2.xml\n",
            "ELTeC-slv/level2/SLV10012-L2.xml\n",
            "ELTeC-slv/level2/SLV10013-L2.xml\n",
            "ELTeC-slv/level2/SLV10014-L2.xml\n",
            "ELTeC-slv/level2/SLV10015-L2.xml\n",
            "ELTeC-slv/level2/SLV10016-L2.xml\n",
            "ELTeC-slv/level2/SLV10017-L2.xml\n",
            "ELTeC-slv/level2/SLV10018-L2.xml\n",
            "ELTeC-slv/level2/SLV10019-L2.xml\n",
            "ELTeC-slv/level2/SLV10020-L2.xml\n",
            "ELTeC-slv/level2/SLV10021-L2.xml\n",
            "ELTeC-slv/level2/SLV10022-L2.xml\n",
            "ELTeC-slv/level2/SLV10023-L2.xml\n",
            "ELTeC-slv/level2/SLV10024-L2.xml\n",
            "ELTeC-slv/level2/SLV10025-L2.xml\n",
            "ELTeC-slv/level2/SLV10026-L2.xml\n",
            "ELTeC-slv/level2/SLV10027-L2.xml\n",
            "ELTeC-slv/level2/SLV10028-L2.xml\n",
            "ELTeC-slv/level2/SLV10029-L2.xml\n",
            "ELTeC-slv/level2/SLV10030-L2.xml\n",
            "ELTeC-slv/level2/SLV10031-L2.xml\n",
            "ELTeC-slv/level2/SLV10032-L2.xml\n",
            "ELTeC-slv/level2/SLV20001-L2.xml\n",
            "ELTeC-slv/level2/SLV20002-L2.xml\n",
            "ELTeC-slv/level2/SLV30001-L2.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r NIF-srp.zip NIF-srp"
      ],
      "metadata": {
        "id": "ro7-h0hlw8R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r NIF-por.zip NIF-por"
      ],
      "metadata": {
        "id": "sXqaHo0CXmwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r NIF-slv.zip NIF-slv"
      ],
      "metadata": {
        "id": "dbik1qQKl5FW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68625a44-1e23-4a19-8c31-a11dcf17d447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: NIF-slv/ (stored 0%)\n",
            "  adding: NIF-slv/SLV00240-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00459-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV00526-L2.txt (deflated 57%)\n",
            "  adding: NIF-slv/SLV00099-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10003-L2.txt (deflated 63%)\n",
            "  adding: NIF-slv/SLV10027-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00111-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00461-L2.txt (deflated 63%)\n",
            "  adding: NIF-slv/SLV10002-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV00310-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00460-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00497-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10011-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00391-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00090-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV10021-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00278-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00483-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00455-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00502-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00231-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10024-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00391-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00048-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV00421-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10009-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00410-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00135-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00345-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10024-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00094-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00112-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00227-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV10018-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00324-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00506-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00456-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV00072-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10020-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00024-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10027-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10021-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00406-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00363-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00126-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10017-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00136-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00132-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00502-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV00194-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00098-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00234-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00103-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00136-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00425-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00455-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV00461-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00473-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV20001-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV10016-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10031-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10005-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00417-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV10008-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00172-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV10025-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00278-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00231-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV10023-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00234-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00098-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10030-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV10015-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10009-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10012-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00279-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00478-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00452-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00126-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10002-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00072-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10025-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00459-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV30001-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV00227-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00279-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV20002-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00496-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV00194-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00111-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10004-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00355-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00135-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV10007-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00058-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10017-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00458-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00099-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00240-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10012-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV10001-L2.txt (deflated 62%)\n",
            "  adding: NIF-slv/SLV10014-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00498-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00478-L2.txt (deflated 56%)\n",
            "  adding: NIF-slv/SLV10030-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00217-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV10006-L2.txt (deflated 63%)\n",
            "  adding: NIF-slv/SLV10029-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00373-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00454-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00273-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV00497-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV10026-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10005-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00452-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00363-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00122-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV10015-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00364-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00310-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00217-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00473-L2.txt (deflated 56%)\n",
            "  adding: NIF-slv/SLV00352-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10006-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00024-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV00216-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00174-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00273-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV20002-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV10028-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10014-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00325-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00476-L2.txt (deflated 57%)\n",
            "  adding: NIF-slv/SLV00398-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00092-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10023-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00454-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00364-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV10031-L2.txt (deflated 62%)\n",
            "  adding: NIF-slv/SLV00456-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00048-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10004-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10020-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00410-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV10019-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV10011-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00352-L2.txt (deflated 56%)\n",
            "  adding: NIF-slv/SLV00483-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV20001-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10007-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10026-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10008-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10029-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00325-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10019-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00094-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10022-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00058-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00406-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV30001-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10013-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV10032-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00417-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00398-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV00401-L2.txt (deflated 63%)\n",
            "  adding: NIF-slv/SLV10010-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00092-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00373-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00458-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV10001-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00355-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00172-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00506-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV10003-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00112-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00090-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00103-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV00421-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10028-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10010-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10022-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00174-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV00476-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10016-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00324-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV10018-L2.txt (deflated 58%)\n",
            "  adding: NIF-slv/SLV00132-L2.txt (deflated 61%)\n",
            "  adding: NIF-slv/SLV00216-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00526-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00345-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00122-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00187-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00498-L2.txt (deflated 59%)\n",
            "  adding: NIF-slv/SLV00496-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00401-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00187-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV10032-L2.txt (deflated 60%)\n",
            "  adding: NIF-slv/SLV00425-L2.ttl (deflated 94%)\n",
            "  adding: NIF-slv/SLV00460-L2.txt (deflated 62%)\n",
            "  adding: NIF-slv/SLV10013-L2.ttl (deflated 94%)\n"
          ]
        }
      ]
    }
  ]
}